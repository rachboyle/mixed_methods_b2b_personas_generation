{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conf_web_analytics_percentage_preprocessing_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPP7jWV+ihWDER2HYMP4Ane",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachboyle/mixed_methods_b2b_personas_generation/blob/main/web_analytics_percentage_preprocessing_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web Analytics Pre-Processing and Sanitization Example "
      ],
      "metadata": {
        "id": "uoEyVTThpDwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urNpFlJKu5ra",
        "outputId": "62a21521-6189-4eb0-d898-8a4be68499d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "#Authenticate to GCP to access survey and web analytics data in BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Iterative Mixed Method Approach to B2B SaaS User Personas Overview\n",
        "Below is an overview of the various analysis and clustering steps we took for our persona generation research. This particular notebook focuses on the step(s) highlighted in yellow: \n",
        "\n",
        "1.   **Survey Data Pre-Processing**: Survey data cleaning and pre-processing\n",
        "2.   **Survey Clustering**: Initial survey clustering to produce rudimentary personas we could use for interview recruitment\n",
        "3.   <mark>**Analytcs Data Pre-Processing**: Web analytics data (from [Pendo](https://www.pendo.io) tool) pre-processing, manipulation, and sanitization tool\n",
        "4.   **Preprocessing and Clustering Iterations**: Four iterations of preprocessing and clustering on the web analytics data; the following iterations listed below contain the clustering algorithm, dimensionality reduction method, and data pre-processing manipulation respectively\n",
        "    * *KMeans, PCA, Averaged Data*\n",
        "    * *KMeans, PCA, Percentage Usage Data Per Day*\n",
        "    * *KMeans, UMAP, Percentage Usage Data Per Day*\n",
        "    * *HDBScan, UMAP, Percentage Usage Data Per Week*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_dAokypcwCbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "kMygO4ECVDhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install UMAP (dimensionality reduction) and HDBscan (clustering algorithm)\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "TPTavOa5yr0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from IPython.display import display_html\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.cm as cm \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "plt.rcParams.update({'figure.max_open_warning': 0}) #set so you do not get warnings\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Dimension reduction and clustering libraries\n",
        "import umap.umap_ as umap\n",
        "import hdbscan\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
        "from sklearn.preprocessing import normalize, StandardScaler"
      ],
      "metadata": {
        "id": "gJFTx6T4_Fcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Pre-Processing Overview\n",
        "We processed the data in three ways: \n",
        "1. **Averages of the raw data**: on a per day average, user A spends __# clicks on feature X, __ mins on page X)\n",
        "2. **Average percentages per day data**: on a per day average, __% of user A's total clicks are spent on feature X, __% of user A's total time is spent on page X)  \n",
        "3. **Average percentages per week data**: on a per week average, __% of user A's total clicks are spent on feature X, __% of user A's total time is spent on page X)\n",
        "\n",
        "Below is an example of how this was done for the last pre-processing method, *average percentages per week data*. The queries listed below are the original ones used and hence reference an email column. This is illustratory, showing how we handled visitor_id <> email pairings and duplicates. However, in the actual clustering iterations email will be removed in queries and outputs for anonymity."
      ],
      "metadata": {
        "id": "jXHPXWt6sSmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Aggregation\n",
        "Aggregate the data by week by assigning each row a year-week id that can then be used to sum event and time data by week per user. Handle duplicate visitors (with the same email)."
      ],
      "metadata": {
        "id": "dhQpO_1KlKGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otnaq34Z3mwz"
      },
      "outputs": [],
      "source": [
        "#Counts to check against when de-duping the email <> visitor_ids \n",
        "%%bigquery --project uxr-design-us-dev duplicates\n",
        "SELECT \"auth visitors w/ duplicate emails\" as type, SUM(cnt) as num\n",
        "FROM (\n",
        "    SELECT distinct email, COUNT(distinct visitor_id) as cnt \n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "    WHERE email is not NULL\n",
        "    GROUP BY email\n",
        "    ORDER BY cnt DESC\n",
        ")\n",
        "WHERE cnt > 1\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"auth visitors with 1 id\" as type, count(email) as num\n",
        "FROM (\n",
        "    SELECT distinct email, COUNT(distinct visitor_id) as cnt \n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "    WHERE email is not NULL\n",
        "    GROUP BY email\n",
        "    ORDER BY cnt DESC\n",
        ")\n",
        "WHERE cnt = 1\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"authenticated visitors\" as type, count(distinct visitor_id) as num\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "WHERE email is not NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"anonymous visitors\" as type, count(distinct visitor_id) as num\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "WHERE email is NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"total visitors counting distinct ids\" as type, count(*) as num\n",
        "FROM (\n",
        "    SELECT distinct visitor_id\n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        ")\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"total visitors counting distinct email <> id\" as type, count(*) as num\n",
        "FROM (\n",
        "    SELECT distinct email, visitor_id\n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        ")\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT \"visitor ids with multiple emails\" as type, count(visitor_id) as num\n",
        "FROM (\n",
        "    SELECT distinct visitor_id, count(distinct email) as cnt\n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "    GROUP BY visitor_id\n",
        ")\n",
        "WHERE cnt > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "aX_9Z-xn3ztB",
        "outputId": "f6c88da7-b4ac-4bfe-88d2-eac1f4d86e2c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anonymous visitors</td>\n",
              "      <td>11085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>total visitors counting distinct ids</td>\n",
              "      <td>34875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>visitor ids with multiple emails</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>authenticated visitors</td>\n",
              "      <td>23790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>auth visitors w/ duplicate emails</td>\n",
              "      <td>809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>auth visitors with 1 id</td>\n",
              "      <td>22982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>total visitors counting distinct email &lt;&gt; id</td>\n",
              "      <td>34876</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           type    num\n",
              "0                            anonymous visitors  11085\n",
              "1          total visitors counting distinct ids  34875\n",
              "2              visitor ids with multiple emails      1\n",
              "3                        authenticated visitors  23790\n",
              "4             auth visitors w/ duplicate emails    809\n",
              "5                       auth visitors with 1 id  22982\n",
              "6  total visitors counting distinct email <> id  34876"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#After investigation, weird edge cases are due to internal testing accounts (i.e. visitor id wth multiple emails)\n",
        "duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxx0uvF43nRV"
      },
      "outputs": [],
      "source": [
        "#Convert Telemetry Data to be Per Week rather than Per Day, consolidate ids\n",
        "%%bigquery --project uxr-design-us-dev features_df\n",
        "with dup_week AS (\n",
        "  SELECT *, CONCAT(EXTRACT(YEAR from day), \"-\", EXTRACT(WEEK from day)) as week_num\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data` \n",
        "  JOIN (\n",
        "    SELECT email as joined_email, MAX(visitor_id) as true_id FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "    WHERE email is not NULL\n",
        "    GROUP BY joined_email\n",
        "  )\n",
        "  ON email = joined_email\n",
        "),\n",
        "\n",
        "nondup_week AS (\n",
        "  SELECT *, CONCAT(EXTRACT(YEAR from day), \"-\", EXTRACT(WEEK from day)) as week_num\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data`\n",
        "  WHERE email is NULL\n",
        ")\n",
        "\n",
        "SELECT app, visitor_id, email, feature_id, feature_name, page_id, page_name,\n",
        "group_name, is_core_event, week_num, SUM(total_num_events) as events_per_week, SUM(total_num_min) as mins_per_week\n",
        "FROM nondup_week\n",
        "GROUP BY app, visitor_id, email, feature_id, feature_name, page_id, page_name, group_name, is_core_event, week_num\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT app, true_id as visitor_id, email, feature_id, feature_name, page_id, page_name,\n",
        "group_name, is_core_event, week_num, SUM(total_num_events) as events_per_week, SUM(total_num_min) as mins_per_week\n",
        "FROM dup_week\n",
        "GROUP BY app, visitor_id, email, feature_id, feature_name, page_id, page_name, group_name, is_core_event, week_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI-tMNx8AF-g"
      },
      "outputs": [],
      "source": [
        "#Check the deduping worked as expected so that the deduped data = anonymous users + authenticated users from the original data\n",
        "%%bigquery --project uxr-design-us-dev deduped_data\n",
        "SELECT \"anonymous users\" as label, COUNT(distinct visitor_id) as num\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data` \n",
        "WHERE email is NULL \n",
        "\n",
        "UNION ALL \n",
        "\n",
        "SELECT \"authenticated users\" as label, COUNT(distinct visitor_id) as num\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data` \n",
        "WHERE email is not NULL\n",
        "\n",
        "UNION ALL \n",
        "\n",
        "SELECT \"deduped data\" as label, COUNT(*) as num\n",
        "FROM (\n",
        "    SELECT distinct email, visitor_id\n",
        "    FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data_per_week_deduped` \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "AoWSmTDLATHs",
        "outputId": "692d1950-7632-4794-dd18-65f7b729d1a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anonymous users</td>\n",
              "      <td>11085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>authenticated users</td>\n",
              "      <td>23307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deduped data</td>\n",
              "      <td>34392</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 label    num\n",
              "0      anonymous users  11085\n",
              "1  authenticated users  23307\n",
              "2         deduped data  34392"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "deduped_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olwNBDnhB58A"
      },
      "outputs": [],
      "source": [
        "#Convert Telemetry Data to be Per Week rather than Per Day, consolidate ids\n",
        "%%bigquery --project uxr-design-us-dev pages_df\n",
        "with dup_week AS (\n",
        "  SELECT *, CONCAT(EXTRACT(YEAR from day), \"-\", EXTRACT(WEEK from day)) as week_num\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_pages_telemetry_data` \n",
        "  JOIN (\n",
        "    SELECT email as joined_email, MAX(visitor_id) as true_id FROM `uxr-design-us-dev.clustering_datasets.all_pages_telemetry_data`\n",
        "    WHERE email is not NULL\n",
        "    GROUP BY joined_email\n",
        "  )\n",
        "  ON email = joined_email\n",
        "),\n",
        "\n",
        "nondup_week AS (\n",
        "  SELECT *, CONCAT(EXTRACT(YEAR from day), \"-\", EXTRACT(WEEK from day)) as week_num\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_pages_telemetry_data`\n",
        "  WHERE email is NULL\n",
        ")\n",
        "\n",
        "SELECT app, visitor_id, email, page_id, page_name,\n",
        "group_name, week_num, SUM(total_num_events) as events_per_week, SUM(total_num_min) as mins_per_week\n",
        "FROM nondup_week\n",
        "GROUP BY app, visitor_id, email, page_id, page_name, group_name, week_num\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT app, true_id as visitor_id, email, page_id, page_name,\n",
        "group_name, week_num, SUM(total_num_events) as events_per_week, SUM(total_num_min) as mins_per_week\n",
        "FROM dup_week\n",
        "GROUP BY app, visitor_id, email, page_id, page_name, group_name, week_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHcD4OrNCPd3"
      },
      "outputs": [],
      "source": [
        "#Save datasets to bigquery for easy reference\n",
        "features_df.to_gbq(\n",
        "    'clustering_datasets.all_features_telemetry_data_per_week_deduped',\n",
        "    'uxr-design-us-dev', \n",
        "     chunksize=None,\n",
        "     if_exists='replace')\n",
        "\n",
        "pages_df.to_gbq(\n",
        "    'clustering_datasets.all_pages_telemetry_data_per_week_deduped',\n",
        "    'uxr-design-us-dev', \n",
        "     chunksize=None,\n",
        "     if_exists='replace')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Conversion\n",
        "Convert the average per week data into percentage per week data."
      ],
      "metadata": {
        "id": "hDsJpIFnQ1kB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMmGUwxLzy3"
      },
      "outputs": [],
      "source": [
        "#Create Per Week Percentage Data for Time on Page\n",
        "%%bigquery --project uxr-design-us-dev page_df\n",
        "WITH total_time_per_week AS (\n",
        "  SELECT visitor_id, week_num, SUM(mins_per_week) as total_mins_per_week\n",
        "  #dataset w/ page interactions averaged per week\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_pages_telemetry_data_per_week_deduped`\n",
        "  GROUP BY visitor_id, week_num \n",
        ")\n",
        "\n",
        "SELECT app, d.visitor_id, t.email, page_id, page_name, t.week_num, mins_per_week as page_mins_per_week, total_mins_per_week,\n",
        "IF(total_mins_per_week = 0, 0, ROUND(mins_per_week / total_mins_per_week,3) * 100) as percent_of_time\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_pages_telemetry_data_per_week_deduped`  t\n",
        "JOIN total_time_per_week d ON d.visitor_id = t.visitor_id AND d.week_num = t.week_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ViXHXhHdUtT"
      },
      "outputs": [],
      "source": [
        "#Create Per Week Percentage Data for Feature Clicks\n",
        "%%bigquery --project uxr-design-us-dev feature_df\n",
        "WITH total_time_per_week AS (\n",
        "  SELECT visitor_id, week_num, SUM(events_per_week) as total_events_per_week\n",
        "  #dataset w/ feature interactions averaged per week\n",
        "  FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data_per_week_deduped` \n",
        "  GROUP BY visitor_id, week_num \n",
        ")\n",
        "\n",
        "\n",
        "SELECT app, d.visitor_id, t.email, feature_id, feature_name, page_id, page_name, t.week_num, events_per_week as feature_events_per_week, total_events_per_week,\n",
        "IF(total_events_per_week = 0, 0, ROUND(events_per_week/ total_events_per_week,3) * 100) as percent_of_events\n",
        "FROM `uxr-design-us-dev.clustering_datasets.all_features_telemetry_data_per_week_deduped`  t\n",
        "JOIN total_time_per_week d ON d.visitor_id = t.visitor_id AND d.week_num = t.week_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BWHeo6qef_C"
      },
      "outputs": [],
      "source": [
        "#Create Average Percentage Data for Feature Clicks\n",
        "%%bigquery --project uxr-design-us-dev ave_feature_df\n",
        "SELECT visitor_id, email, feature_id, feature_name, page_id, page_name, group_name, \n",
        "AVG(feature_events_per_week) as ave_feature_clicks_per_week, AVG(percent_of_events) as ave_percent_of_events_per_week\n",
        "FROM `uxr-design-us-dev.mixed_methods_clustering.feature_percentage_data`\n",
        "GROUP BY visitor_id, email, feature_id, feature_name, page_id, page_name, group_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri3jZcl3yhYp"
      },
      "outputs": [],
      "source": [
        "#Create Average Total Percentage Data for Time on Page\n",
        "%%bigquery --project uxr-design-us-dev ave_page_df\n",
        "SELECT visitor_id, email, page_id, page_name, group_name, \n",
        "AVG(page_mins_per_week) as ave_page_mins_per_week, AVG(percent_of_time) as ave_percent_of_time_per_week\n",
        "FROM `uxr-design-us-dev.mixed_methods_clustering.page_percentage_data`\n",
        "GROUP BY visitor_id, email, page_id, page_name, group_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Sanitization\n",
        "Sanitize the data so it only users that meet the following qualifications:\n",
        "* has logged into the platform in the past year\n",
        "* has been active for the past 10 days\n",
        "* has at least 30 days of activity data\n",
        "* is an external user\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kC8KTcvfQ69Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c0LTRvyRpg",
        "outputId": "19aaae3c-1fd4-4c7c-980b-991852f4ab6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in each dataset before sanitization\n",
            "feature users: 34392\n",
            "page users   : 808423\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of distinct users in each dataset before sanitization\")\n",
        "unique_feature_users = ave_feature_df.drop_duplicates(\"visitor_id\")\n",
        "unique_page_users = ave_page_df.drop_duplicates(\"visitor_id\")\n",
        "\n",
        "print(\"feature users: %d\" % (len(unique_feature_users)))\n",
        "print(\"page users   : %d\" % (len(unique_page_users)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8KJqIH_xu7T"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project uxr-design-us-dev page_users\n",
        "SELECT email, MAX(visitor_id) as visitor_id, MIN(day) as first_login, MAX(day) as most_recent_login, COUNT(distinct day) as total_days_active\n",
        "FROM `clustering_datasets.all_pages_telemetry_data`\n",
        "WHERE email is not NULL\n",
        "GROUP BY email\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT email, visitor_id, MIN(day) as first_login, MAX(day) as most_recent_login, COUNT(distinct day) as total_days_active, \n",
        "FROM `clustering_datasets.all_pages_telemetry_data`\n",
        "WHERE email is NULL\n",
        "GROUP BY email, visitor_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOD4dSRjb-A5",
        "outputId": "4bc115a8-6ac5-410d-e651-8f5634428f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total users in dataset: 808423\n",
            "Total active users: 6342\n",
            "\n",
            "Number of distinct users in each dataset after sanitization\n",
            "feature users: 6287\n",
            "page users   : 6342\n"
          ]
        }
      ],
      "source": [
        "total_unique_users = page_users.drop_duplicates(\"visitor_id\")\n",
        "p_thirty_days_of_activity = page_users[\"most_recent_login\"] - page_users[\"first_login\"] > \"30 days\"\n",
        "p_active_for_10_days = page_users[\"total_days_active\"] >= 10\n",
        "p_login_in_past_year = page_users[\"most_recent_login\"] > \"2020-09-15\"\n",
        "external_users = page_users[\"email\"].str.contains(\"liveramp\") == False\n",
        "active_users = page_users[p_thirty_days_of_activity & p_active_for_10_days & p_login_in_past_year & external_users][[\"visitor_id\", \"email\"]]\n",
        "\n",
        "print(\"\\nNumber of distinct users in each dataset after sanitization\")\n",
        "unique_feature_users = sanitized_ave_feature_df.drop_duplicates(\"visitor_id\")\n",
        "unique_page_users = sanitized_ave_page_df.drop_duplicates(\"visitor_id\")\n",
        "\n",
        "print(\"feature users: %d\" % (len(unique_feature_users)))\n",
        "print(\"page users   : %d\" % (len(unique_page_users)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total users in dataset: %d\\nTotal active users: %d\" % (len(total_unique_users), len(active_users)))\n",
        "sanitized_ave_page_df = ave_page_df[ave_page_df[\"visitor_id\"].isin(active_users[\"visitor_id\"])]\n",
        "sanitized_ave_feature_df = ave_feature_df[ave_feature_df[\"visitor_id\"].isin(active_users[\"visitor_id\"])]"
      ],
      "metadata": {
        "id": "bDNaLCklunzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Consolidation and Transposition\n",
        "Transpose and consolidate the data (where each row is a unique pairing of user <> feature / page <> interaction data) to be in a form suitable for clustering by making each row a datapoint (i.e. user <> all interaction data) and each column a clustering feature (i.e. page X, feature Y). "
      ],
      "metadata": {
        "id": "9dpYbKryRyBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDpWOQB4xcdu",
        "outputId": "1f2c35bb-6ca1-4764-bd8d-bf850ca8ca98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of relevant features: 28\n",
            "\n",
            "ALL FEATURES\n",
            "rows of data: 253605\n",
            "number of features: 423\n",
            "\n",
            "REL FEATURES\n",
            "rows of data: 19248\n",
            "number of features: 28\n"
          ]
        }
      ],
      "source": [
        "#Identify relevant feature ids to use as features in clustering\n",
        "rel_feature_ids = [\"tkt5orHUpCVLIts2gorwT3UgG5I\", \"Ras6hyvLrUgJiRUgon4y1F4QkN4\", \"zdpPTMNp1EkXtBZR5v_ObuE7hwA\", \"C3ryON9ItgJ6UgOwQAZmIE8xa3A\", \"k39G0EasOmyrjXL-x4x8opM1toI\", \"m3O0KeqVvHCNvjUgwnA1gphjwBM\", \"3w9IhLaFCwHOywyY2kj6p7kk21Y\", \"H_1-rGmIvRDj8hnvXi1xwADN5Z4\", \"V1XJLd-R1gdR0eiF5Y7bThnGjHw\", #advancedtv\n",
        "                   \"GID55RO4sNhBCURo7Z1_lY9NLXs\", \"HbEegssf9k60Mv8lnrdv_889PHc\", \"IvuCbXKxt9FCXyAxrnqwFhXPFUM\", \"E_eF9X73IX3ARmERAfLkPPcHMb4\", \"56iulnmaAqdLQOD-wy-EtdOMLxw\", \"UHNiLRNy5-VWEPHlDcq1hAr1DTU\", \"HBzif5IJ9m2SMxfU1JYbpz5hruo\", \"VwQHIEgwiQ88mTKFEiIfk4a8aCk\", #Data Marketplace\n",
        "                   \"dGro_W0obaKWZs8l-xObcxKmhRg\", \"GHdt7hBJpluSyeNAQhuekke04S8\", \"9e_eJvJ3wbmYQlxEI7eadlGc0wA\", \"CUEn5DTA7EtIh2GfrV6n85e-ugg\", \"zyt-c4I5PLjo6We_foZiyC0uU8M\", \"iPN-NrvMGz0wIPCMZV9Vm2CcPyU\", \"uDfqoLTn6mYspzPtPbKZA4wRtTk\", \"LQERdzPDKTwyLqEqUOhRLxc2n_Q\", \"3JO1sgDixw7nC385pm1yyWWHhWw\", \"vkn0gs9bYzBqFjiHc9wDy5FJbE4\", \"myTn0VuzM9FtUyijGCGP6Z-vTiI\" #Connect\n",
        "                   ]\n",
        "print(\"Number of relevant features: %d\\n\" % len(rel_feature_ids))\n",
        "\n",
        "#Original number of features and rows of data\n",
        "f = len(sanitized_ave_feature_df.drop_duplicates(\"feature_id\"))\n",
        "r = len(sanitized_ave_feature_df)\n",
        "\n",
        "sanitized_ave_feature_df = sanitized_ave_feature_df[sanitized_ave_feature_df[\"feature_id\"].isin(rel_feature_ids)]\n",
        "\n",
        "#Check at the difference in the number of rows and number of features before and after selecting those of interest\n",
        "print(\"ALL FEATURES\")\n",
        "print(\"rows of data: %d\\nnumber of features: %d\" % (r, f))\n",
        "print(\"\\nREL FEATURES\")\n",
        "print(\"rows of data: %d\\nnumber of features: %d\" % (len(sanitized_ave_feature_df), len(sanitized_ave_feature_df.drop_duplicates(\"feature_id\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nymlxWNV4dky",
        "outputId": "133484f8-0f69-46b4-a29f-f16ca5137543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of relevant pages: 52\n",
            "\n",
            "ALL PAGES\n",
            "rows of data: 155099\n",
            "number of pages: 185\n",
            "\n",
            "REL PAGES\n",
            "rows of data: 89475\n",
            "number of pages: 52\n"
          ]
        }
      ],
      "source": [
        "#Identify relevant page ids\n",
        "admin_removals = [\"SideCar - Mapping\", \"Audiences\", \"Comopany Settings (All Tabs)\", \"Fast Lane Sources\", \"Customer Users\", \"Company Settings (All Tabs)\"]\n",
        "admin_group_condition = (sanitized_ave_page_df[\"group_name\"] == 'Admin & User Settings') & ~sanitized_ave_page_df[\"page_name\"].isin(admin_removals)\n",
        "\n",
        "dis_keep = [\"Your Destination Accounts\", \"Your Destination Accounts - My Segments\", \"Your Destination Accounts - Edit Account\", \n",
        "            \"Your Destination Accounts - My Segments - Delivery Info\", \"Destinations List\", \"Choose an Integration Group\", \"Configure DA Properties\" ]\n",
        "dis_group_condition = (sanitized_ave_page_df[\"group_name\"] == 'Distribution') & sanitized_ave_page_df[\"page_name\"].isin(dis_keep)\n",
        "\n",
        "dm_removals = [\"Data Seller - Upload New\", \"Data Seller - Upload Existing\", \"Sell Data (All Tabs)\"] #This Sell Data (All Tabs) might be AdvTv\n",
        "dm_group_condition = (sanitized_ave_page_df[\"group_name\"] == 'Data Marketplace') & ~sanitized_ave_page_df[\"page_name\"].isin(dm_removals)\n",
        "\n",
        "ing_group_condition = (sanitized_ave_page_df[\"group_name\"] == 'Ingestion & Dashboard') & (sanitized_ave_page_df[\"page_id\"] != \"bvIuD4FYA6XkH8iev5cRJyiT5qo\") #duplicate page\n",
        "\n",
        "manage_removals = [\"Audience - Users\", \"Audience - Distribution\", \"Audience - Google Tab\", \"Audience Overview\", \"Audience - Imports\"] #extra pages thatt aren't in pendo rn\n",
        "manage_group_condition = (sanitized_ave_page_df[\"group_name\"] == 'Manage Segments') & (~sanitized_ave_page_df[\"page_name\"].isin(manage_removals))\n",
        "\n",
        "adv_tv_keep = ['Activity Report', 'Universes (TV Destination)', 'Taxonomy','Audiences', 'Data Sets', 'Builder', 'OTT Hub']\n",
        "adv_tv_condition = (sanitized_ave_page_df[\"group_name\"].isin(adv_tv_keep)) & (sanitized_ave_page_df[\"page_name\"] != \"OTT Hub - Welcome Page\")\n",
        "\n",
        "#Use all conditions together to create the paired down table\n",
        "final_condition = (adv_tv_condition) | (manage_group_condition) | (ing_group_condition) | (dm_group_condition) | (dis_group_condition) | (admin_group_condition)\n",
        "rel_page_ids = sanitized_ave_page_df[final_condition][\"page_id\"].drop_duplicates().to_list()\n",
        "\n",
        "print(\"Number of relevant pages: %d\\n\" % len(rel_page_ids))\n",
        "\n",
        "#Original number of pages and rows of data\n",
        "f = len(sanitized_ave_page_df.drop_duplicates(\"page_id\"))\n",
        "r = len(sanitized_ave_page_df)\n",
        "\n",
        "sanitized_ave_page_df = sanitized_ave_page_df[sanitized_ave_page_df[\"page_id\"].isin(rel_page_ids)]\n",
        "\n",
        "#Check at the difference in the number of rows and number of pages before and after selecting those of interest\n",
        "print(\"ALL PAGES\")\n",
        "print(\"rows of data: %d\\nnumber of pages: %d\" % (r, f))\n",
        "print(\"\\nREL PAGES\")\n",
        "print(\"rows of data: %d\\nnumber of pages: %d\" % (len(sanitized_ave_page_df), len(sanitized_ave_page_df.drop_duplicates(\"page_id\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEPYLsITBtlr"
      },
      "outputs": [],
      "source": [
        "#Transpose the Dataset\n",
        "\n",
        "#Create dummy tables to feed the transposed dataset  \n",
        "f_cols = [\"visitor_id\", \"email\"] + rel_feature_ids\n",
        "feature_clustering_table = pd.DataFrame([], columns=f_cols)\n",
        "\n",
        "p_cols = [\"visitor_id\", \"email\"] + rel_page_ids\n",
        "page_clustering_table = pd.DataFrame([], columns=p_cols)\n",
        "\n",
        "for index, row in active_users.iterrows():\n",
        "  #FEATURE DATA\n",
        "  holder1 = pd.DataFrame([], columns=f_cols)\n",
        "  visitor_id, email = row\n",
        "\n",
        "  #Create the feature table by grabbing the minute data for each user\n",
        "  feature_visitor_data = sanitized_ave_feature_df[sanitized_ave_feature_df[\"visitor_id\"] == visitor_id][[\"feature_id\", \"ave_percent_of_events_per_week\"]].transpose()\n",
        "\n",
        "  #Make each distinct feature a column and transpose the data\n",
        "  feature_visitor_data.columns = feature_visitor_data.iloc[0]\n",
        "  feature_visitor_data = feature_visitor_data.drop(feature_visitor_data.index[0])\n",
        "\n",
        "  #Add the visitor and email identifiers to the table\n",
        "  feature_visitor_data.insert(0, \"email\", email)\n",
        "  feature_visitor_data.insert(0, \"visitor_id\", visitor_id)\n",
        "\n",
        "  # Add the time data to the page table\n",
        "  holder1[feature_visitor_data.columns] = feature_visitor_data.head(1)[feature_visitor_data.columns]\n",
        "  feature_clustering_table = feature_clustering_table.append(holder1, ignore_index=True)\n",
        "\n",
        "  #PAGE DATA\n",
        "  holder1 = pd.DataFrame([], columns=p_cols)\n",
        "\n",
        "  #Create the page table by grabbing the minute data for each user\n",
        "  page_visitor_data = sanitized_ave_page_df[sanitized_ave_page_df[\"visitor_id\"] == visitor_id][[\"page_id\", \"ave_percent_of_time_per_week\"]].transpose()\n",
        "\n",
        "  #Make each distinct feature a column and transpose the data\n",
        "  page_visitor_data.columns = page_visitor_data.iloc[0]\n",
        "  page_visitor_data = page_visitor_data.drop(page_visitor_data.index[0])\n",
        "\n",
        "  #Add the visitor and email identifiers to the table\n",
        "  page_visitor_data.insert(0, \"email\", email)\n",
        "  page_visitor_data.insert(0, \"visitor_id\", visitor_id)\n",
        "\n",
        "  # Add the time data to the page table\n",
        "  holder1[page_visitor_data.columns] = page_visitor_data.head(1)[page_visitor_data.columns]\n",
        "  page_clustering_table = page_clustering_table.append(holder1, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejsK3F5sJfcC"
      },
      "outputs": [],
      "source": [
        "#Before saving to BQ, reformat the NaNs to 0s and the column headers to be compatible\n",
        "def reformat(df):\n",
        "  df = df.fillna(0)\n",
        "  df[df.columns.tolist()[2::]] = df[df.columns.tolist()[2::]].astype(float) #Takes a while\n",
        "  df.columns = \"_\" + df.columns.str.replace('-', '_')\n",
        "  return df\n",
        "\n",
        "f_df_reformatted = reformat(feature_clustering_table)\n",
        "p_df_reformatted = reformat(page_clustering_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q6CyJagIRP6",
        "outputId": "f3c7edde-39b4-4fde-d641-64d456f1b719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.40s/it]\n",
            "1it [00:05,  5.55s/it]\n"
          ]
        }
      ],
      "source": [
        "f_df_reformatted.to_gbq(\n",
        "    'mixed_methods_clustering.ave_percentage_of_clicks_per_feature_clustering_dataset',\n",
        "    'uxr-design-us-dev', \n",
        "     chunksize=None, # I have tried with several chunk sizes, it runs faster when it's one big chunk (at least for me)\n",
        "     if_exists='replace')\n",
        "\n",
        "p_df_reformatted.to_gbq(\n",
        "    'mixed_methods_clustering.ave_percentage_of_time_on_page_clustering_dataset',\n",
        "    'uxr-design-us-dev', \n",
        "     chunksize=None, # I have tried with several chunk sizes, it runs faster when it's one big chunk (at least for me)\n",
        "     if_exists='replace')\n"
      ]
    }
  ]
}