# Iterative Mixed Method Approach to B2B SaaS User Personas
The various clustering and analysis steps performed in LiveRamp's persona generation research.

## Abstract 
User persona research has predominantly been done on B2C products often to serve sales / marketing teams. However, the application of these approaches on different circumstances to this is still relatively novel. How can we segment users in a way that is both helpful and accurate for a B2B SaaS product and its product development stakeholders, with “imperfect” and obfuscated analytics data? We performed an iterative, mixed qualitative and mixed quantitative approach to user personas that utilized survey data, user interviews, and web analytics data with various preprocessing manipulations to provide a complete picture of the user. Survey data provided insight into who to recruit as well as user background while the interviews provided insight into user journeys, personalities, and pain points. Using UMAP embedding and an iterative, hierarchical, density-based clustering approach informed by qualitative survey and interview insights, we generated clusters based on time per page and clicks per feature from our web analytics data. Diving into each cluster’s top used pages / features enabled us to identify six distinct user personas that align with those deduced from internal tribal knowledge as well as the interviews and surveys from the study. We recommend our approach to user segmentation, an as yet unexplored combination of qualitative and mixed quantitative methods, for the research teams of other mid sized B2B SaaS companies as a way to bridge the gaps in internal user understanding, web analytics data, and disjointed user journeys (across one or multiple product suites) to paint a full, accurate picture of the user.  

## Methodology
### Survey
Our goal with the survey was to capture background information on a user level, give us a high level indication of the personas at play, and help us recruit participants for interviews. We sent out an in-depth, twenty question survey covering aspects such as product use, main goals, task rank and time, role / title, team collaboration, feelings about the product, tools, use of competitor products, etc. 

We utilized our analytics tool to identify a wide variety of users to send our survey to as well as worked with internal teams to generate a list of users outside our UI (i.e. API users). We sent the survey to 1000 randomly selected users from our list, collected around 400 responses, and sanitized these down to the ~250 respondents that completed the survey beginning to end. 

On this sanitized dataset, we performed an initial rudimentary clustering using k-means and principal component analysis. As this was the first formal attempt at quantitative persona creation at our company, we utilized tribal knowledge to inform the K value we picked and let the resulting personas inform our interview recruitment to ensure a fair / equal group selection. 

### User Interviews
With user interviews, we hoped to get a detailed picture of the user’s journey through our B2B product and the important obstacles each user may face. For this reason, we chose an open-ended, loosely-structured style of qualitative interview (Weiss 1994), which consisted of asking participants to summarize their work and explain the obstacles they faced. In this way, participants would naturally discuss their main pain points. 

Users were recruited from a sampling of the pre clustered survey results as well as through our analytics tool to ensure we had a fair / equal representation of our initial hypothetical personas. Around 50 interviews were conducted, each covering questions on the user journey, pain points, user personality, motivations, and collaboration. 

To limit bias, each interview had different interviewers and note takers. The interviews were thus coded (Weiss 1994) by various authors and discussed extensively among all authors, in an iterative process, to derive the main insights and bucket them into broader themes. These themes enabled us to map user journeys from each interview, allowing us to see the overlap and differences between users. From grouping similar journeys, qualitatively-based personas could be illuminated. The exact number of distinct personas could vary depending on how niche or how broad we wanted to be and we looked to results from quantitative methods for insight on what was reasonable. 

### Web Analytics 
#### Overview and Initial Data Preparation
Our web analytics contained user interaction data from two of our platforms, Connect and Advanced TV, captured by our product analytics tool Pendo. This data was a lookback of all interactions over the past six years and consisted of notable columns such as account id, user id, feature id, page id, number of clicks, number of minutes, and (FiveTran sync) timestamp. The data had some prior, irreversible preprocessing applied to it before landing in our environment so we could only see the interactions aggregated per FiveTran sync (interactions summed every few hours per user) rather than raw clickstream data (i.e. user x clicked feature y at timestamp z). The data we selected to use for persona generation from our tool was specifically the number of minutes spent per page and the number of clicks per feature. The total number of records from the page and feature datasets across both platforms was around 20 million.  

Our process of quantitative persona generation was an iterative one, with aspects such as data preprocessing / aggregation, feature selection, clustering algorithm, and clustering parameters being adjusted each iteration as we got feedback from both our qualitative data and internal stakeholders. 

As a first step, we limited the eight years of data we had to be over a two year look back, sanitized out all inactive users, consolidated the datasets of interest into one table, and transformed the data into a form suitable for clustering (i.e. each column represents a clustering feature).

#### Iteration 1: KMeans, PCA, Averaged Data
For the first iteration, the preprocessing we applied to the data was aggregating the time on page and clicks per feature data to be averages per day.  We then, after normalizing, used the same clustering and dimensionality reduction methods we applied to our survey data and selected all page ids and feature ids to be features in our clustering algorithm. Visualizations like the one above showed there was some structure in the data, but diving into the specific details of each cluster showed they were not very discernible from one another. We learned from this iteration that using all possible features (i.e. sign in button) and pages (i.e. welcome page) for our clustering dataset cluttered the data, obscured our understanding of the clusters, and significantly lowered the confidence in our ability to label the clusters as personas for our stakeholders. The dataset being averages of the raw interaction data also made it harder to interpret each cluster in context (i.e. is it significant that users in cluster A spend one more minute on average on page x, click on feature x 2 more times a day). 

#### Iteration 2: KMeans, PCA, Percentage Per Day Data
This iteration kept everything the same as the previous except for the data preprocessing and feature selection (we used only a subset of pages as our clustering features). We aggregated the interaction data by day and created percentages out of it in order to more easily compare across users that spend different amounts of time in the platform and ensure we are not over indexing on that metric in our clustering (i.e. user X spends 50% of their time on page Y per day, 10% on page Z). The results did not seem to have as much structure as our first iteration visually. After reviewing with our internal data science team we uncovered that our data was likely not suited for PCA and KMeans since it is sparse, nonlinear, categorical, and non-normally distributed. But the results seemed to be more promising from a cluster interpretability perspective, with significant and legible differences per cluster that could be used to come up with persona labels. The resulting 8 personas that were generated, though, seemed too granular in comparison to qualitative personas being pieced together from interviews. When diminishing the K value, we would lose clusters representing personas we knew to be separate and distinct from interviews but seemed to not have enough signal to warrant its own grouping in the data.

#### Iteration 3: KMeans, UMAP, Percentage Per Day Data
This iteration kept the preprocessing and clustering algorithm the same, but changed the dimensionality reduction to be one more suited for the sparse, categorical dataset and expanded the clustering features to include core CTAs. We also separated out the data for a persona, tv_users, we identified qualitatively from user interviews and from the previous clustering iteration. Using the new dimensionality reduction method required us to input parameters such as minimum cluster size. Since the persona of tv_users is very small and we did not believe our other personas to be of this size, we decided to handle this small group of users separately so as to not sacrifice the integrity of the clustering iteration by choosing parameters suited to capturing it’s signal. 

With one persona hardcoded, the remaining and resulting six personas seemed very promising, but when communicated to internal stakeholders and teams caused some confusion. The main issue with this iteration was the concept of a “Distributor” persona, who mainly focuses on advertising distribution / activation. This was a sticky point since this task was seen as universal both internally and through the analytics data, a common and perhaps even necessary action to using the platform and not one that would warrant its own, separate persona. It was unclear if this was simply an interpretation / language problem or clustering one.

#### Iteration 4: HDBScan, UMAP, Percentage Per Week Data
By seeking to analyze and address the questions that arose from the prior iteration, we realized that we were dimming certain signals by aggregating the data to be per day since many of our processes take 1- 3 business days and a substantial number of users visit our platform on a weekly basis rather than a daily basis. 

With this iteration, we changed our preprocessing to create percentage data based on weekly interactions (i.e. user X spends 50% of time per week on page Y). We also decided to use HDBScan, a hierarchical density based clustering method that detects the number of clusters since it would be better suited for our dataset and not require us to bias our persona generation by hardcoding the number of clusters. KMeans makes certain assumptions about the qualities of the clusters that our visualizations and analytics showed to not be true, such as the data being “round” or spherical, equally sized, equally dense, most dense in the center of the sphere, and not contaminated by noise/outliers. Like the previous iteration, we removed our already identified cluster of TV users from the dataset. 

With the first pass, we generated four distinct clusters. The largest cluster, which we named “Generalist,” seemed to represent users of our core offering and user experience, which seemed to not have nuances loud enough to warrant a separate cluster within the larger dataset. The other three clusters aligned with those that were already identified from user interviews but were now backed by data and could be further examined. 

We then ran a second pass on just the generalist cluster with the same methodology and found that a sub persona had a strong enough signal to be clustered when just looking at this sample set. This persona was then added to our list, resulting in six total personas. 

With the first pass, we generated four distinct clusters. The largest cluster, which we named “Generalist,” seemed to represent users of our core offering and user experience, which seemed to not have nuances loud enough to warrant a separate cluster within the larger dataset. The other three clusters aligned with those that were already identified from user interviews but were now backed by data and could be further examined. 

We then ran a second pass on just the generalist cluster with the same methodology and found that a sub persona had a strong enough signal to be clustered when just looking at this sample set. This persona was then added to our list, resulting in our six final personas. 
